Day03  Keepalived热备 Keepalived+LVS 、 HAProxy服务器

一.nginx与LVS调度器区别
  1.nginx代理(我帮你去访问)
  2.LVS调度(转发数据包) LVS-NAT LVS-DR LVS-TUN
  # yum install ipvsadm -y
  # ipvsadm -A -t|u VIP:port -s wrr
  # ipvsadm -a -t|u VIP:port -r RIP [-g|m|i]
  # ipvsadm -E|D|C /** VIP **/
  # ipvsadm -e|d  /** RIP **/
  # ipvsadm -Ln  /** 查看 **/
  
  * NAT转发一定要有网关,且RIP不能和client在同网段中
  /** 
  * Keepalived是给LVS写的 
  * 1.自动配置LVS规则,做健康检查
  * 2.Keepalived学习了路由器上的功能VRRP  HSRP路由热备
  **/

二.Keepalived热备
  1.Keepalived概述
    1) Keepalived概述
    · 调度器出现单点故障,如何解决?
    · Keepalived实现了高可用集群
    · Keepalived最初是为LVS设计的,专门监控各服务器节点的状态
    · Keepalived后来加入了VRRP功能,防止单点故障
    2) Keepalived运行原理
    · Keepalived检测每个服务器节点状态
    · 服务器节点异常或工作出现故障,Keepalived将故障节点从集群系统中剔除
    · 故障节点恢复后,Keepalived再将其加入到集群系统中
    · 所有工作自动完成,无需人工干预
  2.Keepalived服务
  /**
  * 准备三台 Linux服务器,两台做Web服务器,并部署Keepalived高可用软件,一台作为客户端
  * 使用Keepalived实现web服务器的高可用
  * proxy服务器的IP地址为192.168.4.5
  * web服务器IP地址分别为192.168.4.100和192.168.4.200
  * web服务器的浮动VIP为192.168.4.80
  * 客户端通过访问VIP地址访问web页面
  **/
    1) Keepalived安装
    /** web1和web2 **/
    # yum install keepalived
    /** 装包 **/
    # vim /etc/keepalived/keepalived.conf
    global_defs {
      notification_email {
        admin@tarena.com.cn                //设置报警收件人邮箱
      }
      notification_email_from ka@localhost    //设置发件人
      smtp_server 127.0.0.1                //定义邮件服务器
      smtp_connect_timeout 30
      router_id  web1                        //设置路由ID号（实验需要修改）
    }
    vrrp_instance VI_1 {
      state MASTER                         //主服务器为MASTER（备服务器需要修改为BACKUP）
      interface eth0                    //定义网络接口
      virtual_router_id 50                //主备服务器VRID号必须一致
      priority 100                     //服务器优先级,优先级高优先获取VIP（实验需要修改）
      advert_int 1
      authentication {
        auth_type pass
        auth_pass 1111                       //主备服务器密码必须一致
      }
      virtual_ipaddress { 192.168.4.80  }    //谁是主服务器谁获得该VIP（实验需要修改）
    }
    /** 配置 **/
    # systemctl restart keepalived && iptables -F
    /** 起服务 **/
    * 当web1的keepalived服务停止时,192.168.4.80会自动切换到web2上
    /** 注意
    * keepalived只监控服务本身,当网卡接口停止时,需要停止keepalived服务VIP才会切换
    **/

三.Keepalived+LVS服务器
  1.问题
  · 使用Keepalived为LVS调度器提供高可用功能,防止调度器单点故障,为用户提供Web服务
    * LVS1调度器真实IP地址为192.168.4.5
    * LVS2调度器真实IP地址为192.168.4.6
    * 服务器VIP地址设置为192.168.4.100,192.168.4.200
    * 使用加权轮询调度算法,真实iweb服务器权重不同
  · 机器配备:
    client eth0(192.168.4.10/24)
    proxy1 eth0(192.168.4.5/24)
    proxy2 eth0(192.168.4.6/24)
    web1 eth0(192.168.4.100/24)
    web2 eth0(192.168.4.200/24)
  2.操作
    1) 设置web1服务器的网络参数
    2) 为web1配置VIP地址lo:0(子网掩码必须是32)
    3) 重启网络服务,设置防火墙与SELinux
    4) 给web2重复上面操作
    5) 配置proxy1与proxy2网络参数(VIP由keepalived自动配置)
    6) 调度器安装Keepalived和ipvsadm
  3.部署Keepalived实现LVS-DR模式调度器的高可用
    1) LVS1调度器设置Keepalived，并启动服务
    /** 上接VIP **/
    virtual_server 192.168.4.15 80 {           //设置ipvsadm的VIP规则（实验需要修改）
      delay_loop 6
      lb_algo wrr                          //设置LVS调度算法为WRR
      lb_kind DR                               //设置LVS的模式为DR
      #persistence_timeout 50
    #注意这样的作用是保持连接，开启后，客户端在一定时间内始终访问相同服务器
      protocol TCP
      real_server 192.168.4.100 80 {         //设置后端web服务器真实IP（实验需要修改）
        weight 1                             //设置权重为1
        TCP_CHECK {                            //对后台real_server做健康检查
        connect_timeout 3
        nb_get_retry 3
        delay_before_retry 3
        }
      }
     real_server 192.168.4.200 80 {       //设置后端web服务器真实IP（实验需要修改）
        weight 2                          //设置权重为2
        TCP_CHECK {
        connect_timeout 3
        nb_get_retry 3
        delay_before_retry 3
        }
      }
    }
    /** 健康检查种类 **/
    real server {
    	TCP_CHECK{
	  connect_timeout 3 /** 超时时间 **/
	  nb_get_retry 3 /** 尝试3次 **/
	  delay_before_retry 3 /** 每3秒重试一次 **/
	} /** 以端口可用性为依据 **/
	HTTP_CHECK{} /** 以网页为依据 **/
	SSL_GET{
	  path /
	  digest md5sum /** 以md5码为验证 **/
	}  /** 以网页为依据 **/
    }

四.配置HAProxy负载均衡集群
  1.调度器宏观对比
    调度器种类Nginx,LVS,HAProxy,F5 big-ip
    
    速度:F5 > LVS > HAProxy > Nginx
    * LVS 4层调度,不支持7层
    * HAProxy 有一定的功能,速度还不错,对正则的支持不如Nginx
      HAProxy既能做4层调度,也能做7层调度
    * Nginx 4,7层调度器
      4.9以后支持4层调度,但还不够成熟
  2.准备4台Linux服务器,两台做web服务器,1台安装HAproxy,1台做客户端,实现以下功能
    · 客户端访问HAProxy,HAProxy分发请求到后端Real Server
    · 开启HAProxy监控页面,及时查看调度器状态
    · 设置HAProxy为开机启动
  3.清除前面的实验环境
    · web1/web2:
    # rm -f network/ifcfg-lo:0
    # vim /etc/sysctl.conf
    # ifdown eth0
    # systemctl restart network
    · proxy1/proxy2
    # systemctl stop keepalived.service
  4.配置后端服务器
    · 配置web1/2的httpd
    · 在proxy1上安装HAProxy
    # yum install haproxy -y
    · 修改配置文件
    # vim /etc/haproxy/haproxy.cfg
    
    global
     log 127.0.0.1 local2   ###[err warning info debug]
     chroot /usr/local/haproxy
     pidfile /var/run/haproxy.pid ###haproxy的pid存放路径
     maxconn 4000     ###最大连接数，默认4000
     user haproxy
     group haproxy
     daemon       ###创建1个进程进入deamon模式运行
    defaults
     mode http    ###默认的模式mode { tcp|http|health } log global   ###采用全局定义的日志
     option dontlognull  ###不记录健康检查的日志信息
     option httpclose  ###每次请求完毕后主动关闭http通道
     option httplog   ###日志类别http日志格式
     option forwardfor  ###后端服务器可以从Http Header中获得客户端ip
     option redispatch  ###serverid服务器挂掉后强制定向到其他健康服务器
     timeout connect 10000 #如果backend没有指定，默认为10s
     timeout client 300000 ###客户端连接超时
     timeout server 300000 ###服务器连接超时
     maxconn  60000  ###最大连接数
     retries  3   ###3次连接失败就认为服务不可用，也可以通过后面设置
    listen stats
        bind 0.0.0.0:1080   #监听端口
        stats refresh 30s   #统计页面自动刷新时间
        stats uri /stats   #统计页面url
        stats realm Haproxy Manager #统计页面密码框上提示文本
        stats auth admin:admin  #统计页面用户名和密码设置
      #stats hide-version   #隐藏统计页面上HAProxy的版本信息
    listen  websrv-rewrite 0.0.0.0:80
       balance roundrobin
       server  web1 192.168.2.100:80 check inter 2000 rise 2 fall 5
       server  web2 192.168.2.200:80 check inter 2000 rise 2 fall 5
    
    参数解析
    global  /** 全局设置 **/
      maxconn=2000  [全局并发量]
    defaults  /** 默认设置 **/
      maxconn=1000  [默认并发量]
    listen集群  /** 单集群设置 **/
      maxconn=500  [单集群并发]
    5.两种方式定义集群:
      frontend+backend
      listen

      listen xxx *:80
        server ip1
        server ip2
	server  web1 192.168.2.100:80 check inter 2000 rise 2 fall 5  /** 健康检查:5次算失败,每20s再次检查,成功两次后放回集群 **/
      listen status 0.0.0.0:1080
        stats refresh 30s
	stats uri /stats
	stats realm HAProxy Manager
	stats auth admin:admin
    6.帮助文档
    # ls /usr/share/doc/haproxy-1.5.18/configuration.txt 
